{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spaghetti!\n",
      "Python 3.7.11\n",
      "Num GPUs Available 0\n",
      "tf.Tensor(-1327.9735, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 13:02:33.675715: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-04 13:02:33.677361: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "print('Spaghetti!')\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "!python --version\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available\", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Food recognition\n",
    "import PIL\n",
    "from os import listdir\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local data directories\n",
    "DATA_DIR = '/Users/dj/Documents/GitHub/Data/food-recognition-challenge-2021'\n",
    "TRAIN_DIR = DATA_DIR + '/train_set/train_set/'\n",
    "TEST_DIR = DATA_DIR + '/test_set/test_set/'\n",
    "\n",
    "# Maximum number of images to load (there are 30k)\n",
    "MAX_IMAGE = 200\n",
    "\n",
    "# Filter for a subset of labels in the images to load\n",
    "FILTER_LABEL = ['1', '2', '3', '4', '5']\n",
    "\n",
    "# load all the training labels\n",
    "train_labels = pd.read_csv(DATA_DIR + '/train_labels.csv', dtype={'label': object})\n",
    "if (len(FILTER_LABEL) > 0):\n",
    "    train_labels = train_labels[train_labels['label'].isin(FILTER_LABEL)].copy().reset_index()\n",
    "print('{} training labels loaded'.format(len(train_labels)))  \n",
    "\n",
    "# load all the training images\n",
    "train_images = list()\n",
    "imcount = 0\n",
    "if MAX_IMAGE == 0:\n",
    "    MAX_IMAGE = len(train_labels)\n",
    "\n",
    "print('Start loading {} images'.format(MAX_IMAGE))\n",
    "for filename in train_labels['img_name']:\n",
    "    # Load image\n",
    "    img = Image.open(TRAIN_DIR + filename)\n",
    "    \n",
    "    # Resize image to the same shape\n",
    "    # TODO: Don't squash all images into the same size\n",
    "    #img = img.resize((256,256), Image.ANTIALIAS)\n",
    "\n",
    "    # Convert image to numpy array\n",
    "    img_data = np.array(img)\n",
    "\n",
    "    # Store loaded image\n",
    "    train_images.append(img_data)\n",
    "\n",
    "    # Show progress\n",
    "    if imcount % 100 == 0:\n",
    "        print('Loading at {:.1f}%'.format(imcount/min(len(train_labels), MAX_IMAGE)*100))\n",
    "\n",
    "    # Stop after X images   \n",
    "    imcount += 1\n",
    "    if imcount > MAX_IMAGE:\n",
    "        print('Reached MAX_IMAGE count of '+str(MAX_IMAGE))\n",
    "        break\n",
    "\n",
    "# Show 10 samples\n",
    "for i in range(0,min(10, len(train_labels))):\n",
    "    print(train_labels['label'][i])\n",
    "    print(train_labels['img_name'][i])\n",
    "    pyplot.imshow(train_images[i])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a training and test set generator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_split = int(len(train_labels)*0.8)\n",
    "train_dataframe = train_labels[:train_split][['img_name','label']]\n",
    "val_dataframe = train_labels[train_split:][['img_name','label']]\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_dataframe,\n",
    "    x_col = 'img_name',\n",
    "    y_col = 'label',\n",
    "    directory=TRAIN_DIR,\n",
    "    target_size=(256,256),\n",
    "    batch_size=25,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_dataframe,\n",
    "    x_col = 'img_name',\n",
    "    y_col = 'label',\n",
    "    directory=TRAIN_DIR,\n",
    "    target_size=(256,256),\n",
    "    batch_size=25,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "base_model = ResNet50(input_shape = (255, 255, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "base_model = Sequential()\n",
    "base_model.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n",
    "base_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "base_model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "\n",
    "resnet_history = base_model.fit(train_generator, validation_data = val_generator, steps_per_epoch = 5, epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seth and Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "base_model = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "base_model = Sequential()\n",
    "base_model.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n",
    "base_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "base_model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "\n",
    "resnet_history = base_model.fit(train_generator, validation_data = val_generator, steps_per_epoch = 15, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
